[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Méthodes par arbres",
    "section": "",
    "text": "Présentation\nCe tutoriel présente quelques exercices d’application sur les méthodes par arbres. On pourra trouver\n\nles supports de cours associés à ce tutoriel ainsi que les données utilisées à l’adresse suivante https://lrouviere.github.io/page_perso/grande_dim.html ;\nle tutoriel sans les corrections à l’url https://lrouviere.github.io/TUTO_ARBRES/\nle tutoriel avec les corrigés (à certains moment) à l’url https://lrouviere.github.io/TUTO_ARBRES/correction.\n\nIl est recommandé d’utiliser mozilla firefox pour lire le tutoriel.\nDes connaissances de base en R et en statistique (modèles de régression) sont nécessaires. Le tutoriel se structure en 4 parties :\n\nArbres : construction d’arbres et élagages avec rpart\nForêts aléatoires : l’algorithme et le choix des paramètres avec ranger et randomForest\nGradient boosting: l’algorithme et le choix des paramètres avec gbm et xgboost"
  },
  {
    "objectID": "01-arbres.html#coupures-cart-en-fonction-de-la-nature-des-variables",
    "href": "01-arbres.html#coupures-cart-en-fonction-de-la-nature-des-variables",
    "title": "1  Méthodes CART",
    "section": "1.1 Coupures CART en fonction de la nature des variables",
    "text": "1.1 Coupures CART en fonction de la nature des variables\nUne partition CART s’obtient en séparant les observations en 2 selon une coupure parallèle aux axes puis en itérant ce procédé de séparation binaire sur les deux groupes… Par conséquent la première question à se poser est : pour un ensemble de données \\((x_1,y_1),\\dots,(x_n,y_n)\\) fixé, comment obtenir la meilleure coupure ?\nComme souvent ce sont les données qui vont répondre à cette question. La sélection de la meilleur coupure s’effectue en introduisant une fonction d’impureté \\(\\mathcal I\\) qui va mesurer le degrés d’hétérogénéité d’un nœud \\(\\mathcal N\\). Cette fonction prendra de\n\ngrandes valeurs pour les nœuds hétérogènes (les valeurs de \\(Y\\) diffèrent à l’intérieur du nœud) ;\nfaibles valeurs pour les nœuds homogènes (les valeurs de \\(Y\\) sont proches à l’intérieur du nœud).\n\nOn utilise souvent comme fonction d’impureté :\n\nla variance en régression\n\\[\\mathcal I(\\mathcal N)=\\frac{1}{|\\mathcal N|}\\sum_{i:x_i\\in\\mathcal N}(y_i-\\overline{y}_{\\mathcal N})^2,\\] où \\(\\overline{y}_{\\mathcal N}\\) désigne la moyenne des \\(y_i\\) dans \\(\\mathcal N\\).\nl’impureté de Gini en classification binaire\n\\[\\mathcal I(\\mathcal N)=2p(\\mathcal N)(1-p(\\mathcal N))\\] où \\(p(\\mathcal N)\\) représente la proportion de 1 dans \\(\\mathcal N\\).\n\nLes coupures considérées par l’algorithme CART sont des hyperplans orthogonaux aux axes de \\(\\mathbb R^p\\), choisir une coupure revient donc à choisir une variable \\(j\\) parmi les \\(p\\) variables explicatives et un seuil \\(s\\) dans \\(\\mathbb R\\). On peut donc représenter une coupure par un couple \\((j,s)\\). Une fois l’impureté définie, on choisira la coupure \\((j,s)\\) qui maximise le gain d’impureté entre le noeud père et ses deux noeuds fils : \\[\\Delta(\\mathcal I)=\\mathbf P(\\mathcal N)\\mathcal I(\\mathcal N)-(\\mathbf P(\\mathcal N_1(j,s))\\mathcal I(\\mathcal N_1(j,s))+\\mathbf P(\\mathcal N_2(j,s))\\mathcal I(\\mathcal N_2(j,s))\\] où\n\n\\(\\mathcal N_1(j,s)\\) et \\(\\mathcal N_2(j,s)\\) sont les 2 nœuds fils de \\(\\mathcal N\\) engendrés par la coupure \\((j,s)\\) ;\n\\(\\mathbf P(\\mathcal N)\\) représente la proportion d’observations dans le nœud \\(\\mathcal N\\).\n\n\n1.1.1 Arbres de régression\nOn considère le jeu de données suivant où le problème est d’expliquer la variable quantitative \\(Y\\) par la variable quantitative \\(X\\).\n\nn &lt;- 50\nset.seed(1234)\nX &lt;- runif(n)\nset.seed(5678)\nY &lt;- 1*X*(X&lt;=0.6)+(-1*X+3.2)*(X&gt;0.6)+rnorm(n,sd=0.1)\ndata1 &lt;- data.frame(X,Y)\nggplot(data1)+aes(x=X,y=Y)+geom_point()\n\n\n\n\n\nA l’aide de la fonction rpart du package rpart, construire un arbre permettant d’expliquer \\(Y\\) par \\(X\\).\n\nlibrary(rpart)\n\nVisualiser l’arbre à l’aide des fonctions prp et rpart.plot du package rpart.plot.\n\nlibrary(rpart.plot)\n\nÉcrire l’estimateur associé à l’arbre.\n\nOn a un modèle de régression\n\\[Y=m(X)+\\varepsilon\\]\noù la fonction de régression (inconnue) \\(m(x)\\) est estimée par\n\\[\\widehat m(x)=0.31\\, \\mathbf{1}_{x\\lt 0.58}+2.4\\,\\mathbf{1}_{x\\geq 0.58}.\\]\n\nAjouter sur le graphe de la question 1 la partition définie par l’arbre ainsi que les valeurs prédites.\n\nOn obtient une partition avec 2 nœuds terminaux. Cette partition peut être résumée par la question : “est-ce que \\(X\\) est plus petit que 0.58 ?”.\n\n\n\n\n1.1.2 Arbres de classification\nOn considère les données suivantes où le problème est d’expliquer la variable binaire \\(Y\\) par deux variables quantitatives \\(X_1\\) et \\(X_2\\).\n\nn &lt;- 50\nset.seed(12345)\nX1 &lt;- runif(n)\nset.seed(5678)\nX2 &lt;- runif(n)\nY &lt;- rep(0,n)\nset.seed(54321)\nY[X1&lt;=0.45] &lt;- rbinom(sum(X1&lt;=0.45),1,0.85)\nset.seed(52432)\nY[X1&gt;0.45] &lt;- rbinom(sum(X1&gt;0.45),1,0.15)\ndata2 &lt;- data.frame(X1,X2,Y)\nggplot(data2)+aes(x=X1,y=X2,color=Y)+geom_point(size=2)+\n  scale_x_continuous(name=\"\")+\n  scale_y_continuous(name=\"\")\n\n\n\n\n\nConstruire un arbre permettant d’expliquer \\(Y\\) par \\(X_1\\) et \\(X_2\\). Représenter l’arbre et identifier l’éventuel problème.\n\nOn observe que l’arbre construit est un arbre de régression, pas de classification. Cela vient du fait que \\(Y\\) est considérée par R comme une variable quantitative, il faut la convertir en facteur.\n\n\nTout est OK maintenant !\n\nÉcrire la règle de classification ainsi que la fonction de score définies par l’arbre.\n\nLa règle de classification est\n\\[\\widehat g(x)=\\mathbf{1}_{X_1\\lt 0.44}.\\]\nLa fonction de score est donnée par\n\\[\\widehat S(x)=\\widehat P(Y=1|X=x)=0.83\\mathbf{1}_{X_1\\lt 0.44}+0.07\\mathbf{1}_{X_1\\geq 0.44}.\\]\n\nAjouter sur le graphe de la question 1 la partition définie par l’arbre.\n\n\n\n1.1.3 Entrée qualitative\nOn considère les données\n\nn &lt;- 100\nX &lt;- factor(rep(c(\"A\",\"B\",\"C\",\"D\"),n))\nset.seed(1234)\nY[X==\"A\"] &lt;- rbinom(sum(X==\"A\"),1,0.9)\nY[X==\"B\"] &lt;- rbinom(sum(X==\"B\"),1,0.25)\nY[X==\"C\"] &lt;- rbinom(sum(X==\"C\"),1,0.8)\nY[X==\"D\"] &lt;- rbinom(sum(X==\"D\"),1,0.2)\nY &lt;- as.factor(Y)\ndata3 &lt;- data.frame(X,Y)\n\n\nConstruire un arbre permettant d’expliquer \\(Y\\) par \\(X\\).\nExpliquer la manière dont l’arbre est construit dans ce cadre là.\n\nLa variable étant qualitative, on ne cherche pas un seuil de coupure pour diviser un nœud en 2. On va ici considérer toutes les partitions binaires de l’ensemble \\(\\{A,B,C,D\\}\\). La meilleure partition est \\(\\{\\{A,C\\},\\{B,D\\}\\}\\)."
  },
  {
    "objectID": "01-arbres.html#sec-elagage-cart",
    "href": "01-arbres.html#sec-elagage-cart",
    "title": "1  Méthodes CART",
    "section": "1.2 Élagage",
    "text": "1.2 Élagage\nLe procédé de coupe présenté précédemment permet de définir un très grand nombre d’arbres à partir d’un jeu de données (arbre sans coupure, avec une coupure, deux coupures…). Se pose alors la question de trouver le meilleur arbre parmi tous les arbres possibles. Une première idée serait de choisir parmi tous les arbres possibles celui qui optimise un critère de performance. Cette approche, bien que cohérente, n’est généralement pas possible à mettre en œuvre en pratique car le nombre d’arbres à considérer est souvent trop important.\nLa méthode CART propose une procédure permettant de choisir automatiquement un arbre en 3 étapes :\n\nOn construit un arbre maximal (très profond) \\(\\mathcal T_{max}\\) ;\nOn sélectionne une suite d’arbres emboités : \\[\\mathcal T_{max}=\\mathcal T_0\\supset\\mathcal T_1\\supset\\dots\\supset \\mathcal T_K.\\] La sélection s’effectue en optimisant un critère Cout/complexité qui permet de réguler le compromis entre ajustement et complexité de l’arbre.\nOn sélectionne un arbre dans cette sous-suite en optimisant un critère de performance.\n\nCette approche revient à choisir un sous-arbre de l’arbre \\(\\mathcal T_\\text{max}\\), c’est-à-dire à enlever des branches à \\(T_\\text{max}\\), c’est pourquoi on parle d’élagage.\n\n1.2.1 Élagage pour un problème de régression\nOn considère les données Carseats du package ISLR.\n\nlibrary(ISLR)\ndata(Carseats)\nsummary(Carseats)\n\n     Sales          CompPrice       Income        Advertising    \n Min.   : 0.000   Min.   : 77   Min.   : 21.00   Min.   : 0.000  \n 1st Qu.: 5.390   1st Qu.:115   1st Qu.: 42.75   1st Qu.: 0.000  \n Median : 7.490   Median :125   Median : 69.00   Median : 5.000  \n Mean   : 7.496   Mean   :125   Mean   : 68.66   Mean   : 6.635  \n 3rd Qu.: 9.320   3rd Qu.:135   3rd Qu.: 91.00   3rd Qu.:12.000  \n Max.   :16.270   Max.   :175   Max.   :120.00   Max.   :29.000  \n   Population        Price        ShelveLoc        Age          Education   \n Min.   : 10.0   Min.   : 24.0   Bad   : 96   Min.   :25.00   Min.   :10.0  \n 1st Qu.:139.0   1st Qu.:100.0   Good  : 85   1st Qu.:39.75   1st Qu.:12.0  \n Median :272.0   Median :117.0   Medium:219   Median :54.50   Median :14.0  \n Mean   :264.8   Mean   :115.8                Mean   :53.32   Mean   :13.9  \n 3rd Qu.:398.5   3rd Qu.:131.0                3rd Qu.:66.00   3rd Qu.:16.0  \n Max.   :509.0   Max.   :191.0                Max.   :80.00   Max.   :18.0  \n Urban       US     \n No :118   No :142  \n Yes:282   Yes:258  \n                    \n                    \n                    \n                    \n\n\nOn cherche ici à expliquer la variable quantitative Sales par les autres variables.\n\nConstruire un arbre permettant de répondre au problème.\nExpliquer les sorties de la fonction printcp appliquée à l’arbre de la question précédente et calculer le dernier terme de la colonne rel error.\n\nOn peut lire des informations sur la suite d’arbres emboîtés, cette suite est de longueur 17 ici. Dans le dernier tableau, chaque ligne représente un arbre de la suite et on a dans les colonnes :\n\nCP : le paramètre de complexité, plus il est petit plus l’arbre est profond ;\nnsplit : nombre de coupures de l’arbre ;\nrel error contient l’erreur calculée sur les données d’apprentissage. Cette erreur décroit lorsque la complexité augmente et peut être interprétée comme une erreur d’ajustement ;\nxerror : contient l’erreur calculée par validation croisée. Elle peut être interprétée comme une erreur de prévision ;\nxstd correspond à l’écart type estimé de l’erreur.\n\nLes types d’erreurs dépendent du problème considéré. Vu qu’on est ici sur un problème de régression, c’est l’erreur quadratique moyenne qui est considérée. De plus ces erreurs sont normalisées par rapport à l’erreur de l’arbre racine (sans coupure). Ainsi on retrouve l’erreur demandée avec\n\nConstruire une suite d’arbres plus grandes en jouant sur les paramètres cp et minsplit de la fonction rpart.\n\nIl suffit de diminuer les valeurs par défaut de ces paramètres.\n\n\nOn obtient ici une suite de près de 300 arbres. On remarque que\n\nl’erreur d’ajustement ne cesse de décroître, ceci est logique vu le procédé de construction : on ajuste de mieux en mieux lorsqu’on augmente le nombre de coupures ;\nl’erreur de prévision décroit avant de d’augmenter à nouveau. C’est le phénomène bien connu du sur-apprentissage.\n\n\nExpliquer la sortie de la fonction plotcp appliquée à l’arbre de la question précédente.\n\nOn obtient un graphe qui permet de visualiser l’erreur quadratique calculée par validation croisée (erreur de prévision) en fonction du paramètre cp ou nsplit.\n\nSélectionner le “meilleur” arbre dans la suite construite.\n\nLa manière classique revient à choisir l’arbre qui a la plus petite erreur de prévision. Cela revient à aller chercher dans le tableau de la fonction printcp l’arbre qiu possède la plus petite erreur de prévision. On peut obtenir la valeur optimale de cp avec\n\nVisualiser l’arbre choisi (utiliser la fonction prune).\n\nLa fonction visTree du package visNetwork permet de donner une visualisation interactive de l’arbre.\n\n\nUne application Shiny est également proposée pour visualiser les arbres\n\nOn souhaite prédire les valeurs de \\(Y\\) pour de nouveaux individus à partir de l’arbre sélectionné. Pour simplifier on considèrera ces 4 individus :\n\n(new_ind &lt;- Carseats |&gt; as_tibble() |&gt; \n   slice(3,58,185,218) |&gt; dplyr::select(-Sales))\n\n# A tibble: 4 × 10\n  CompPrice Income Advertising Population Price ShelveLoc   Age Education Urban\n      &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;\n1       113     35          10        269    80 Medium       59        12 Yes  \n2        93     91           0         22   117 Bad          75        11 Yes  \n3       132     33           7         35    97 Medium       60        11 No   \n4       106     44           0        481   111 Medium       70        14 No   \n# ℹ 1 more variable: US &lt;fct&gt;\n\n\nCalculer les valeurs prédites.\nSéparer les données en un échantillon d’apprentissage de taille 250 et un échantillon test de taille 150.\nOn considère la suite d’arbres définie par\n\nset.seed(4321)\ntree &lt;- rpart(Sales~.,data=train,cp=0.000001,minsplit=2)\n\nDans cette suite, sélectionner\n\nun arbre très simple (avec 2 ou 3 coupures)\nun arbre très grand\nl’arbre optimal (avec la procédure d’élagage classique).\n\nCalculer l’erreur quadratique de ces 3 arbres en utilisant l’échantillon test.\n\nPour chaque arbre \\(T\\) on calcule \\[\\frac{1}{n_\\text{test}}\\sum_{i\\in \\text{test}}(Y_i-T(X_i))^2.\\]\nOn définit une table qui regroupe les prédictions des 3 arbres sur l’échantillon test :\n\n\nOn en déduit les erreurs quadratique\n\n\nL’arbre sélectionné a ici la plus petite erreur.\n\nRefaire la comparaison avec une validation croisée 10 blocs.\n\nOn créé tout d’abord les blocs.\n\n\nOn fait la validation croisée.\n\n\n\n\n1.2.2 Élagage en classification binaire et matrice de coût\nOn considère ici les mêmes données que précédemment mais on cherche à expliquer une version binaire de la variable Sales. Cette nouvelle variable, appelée High prend pour valeurs No si Sales est inférieur ou égal à 8, Yes sinon. On travaillera donc avec le jeu data1 défini ci-dessous.\n\nHigh &lt;- ifelse(Carseats$Sales&lt;=8,\"No\",\"Yes\")\ndata1 &lt;- Carseats |&gt; dplyr::select(-Sales) |&gt; mutate(High)\n\n\nConstruire un arbre permettant d’expliquer High par les autres variables (sans Sales évidemment !) et expliquer les principales différences par rapport à la partie précédente précédente.\n\nL’arbre construit est un arbre de classification. Le procédé de découpe des noeuds est différent : il utilise l’impureté de Gini au lieu de la variance.\n\nExpliquer l’option parms dans la commande :\n\ntree1 &lt;- rpart(High~.,data=data1,parms=list(split=\"information\"))\ntree1$parms\n\n$prior\n   1    2 \n0.59 0.41 \n\n$loss\n     [,1] [,2]\n[1,]    0    1\n[2,]    1    0\n\n$split\n[1] 2\n\n\n\nOn change de fonction d’impureté (information au lieu de Gini).\n\nExpliquer les sorties de la fonction printcp sur le premier arbre construit et retrouver la valeur du dernier terme de la colonne rel error.\n\nOn peut lire des informations sur la suite d’arbres emboîtés, cette suite est de longueur 17 ici. Dans le dernier tableau, chaque ligne représente un arbre de la suite et on a dans les colonnes :\n\nCP : le paramètre de complexité, plus il est petit plus l’arbre est profond ;\nnsplit : nombre de coupures de l’arbre ;\nrel error contient l’erreur calculée sur les données d’apprentissage. Cette erreur décroit lorsque la complexité augmente et peut être interprétée comme une erreur d’ajustement ;\nxerror : contient l’erreur calculée par validation croisée. Elle peut être interprétée comme une erreur de prévision ;\nxstd correspond à l’écart type estimé de l’erreur.\n\nLes types d’erreurs dépendent du problème considéré. Vu qu’on est ici sur un problème de classification, c’est l’erreur de classification qui est considérée. De plus ces erreurs sont normalisées par rapport à l’erreur de l’arbre racine (sans coupure). Ainsi on retrouve l’erreur demandée avec\n\nSélectionner un arbre optimal dans la suite.\nOn considère la suite d’arbres\n\ntree2 &lt;- rpart(High~.,data=data1,\n               parms=list(loss=matrix(c(0,5,1,0),ncol=2)),\n               cp=0.01,minsplit=2)\n\nExpliquer les sorties des commandes suivantes. On pourra notamment calculer le dernier terme de la colonne rel error de la table cptable.\n\ntree2$parms\n\n$prior\n   1    2 \n0.59 0.41 \n\n$loss\n     [,1] [,2]\n[1,]    0    1\n[2,]    5    0\n\n$split\n[1] 1\n\nprintcp(tree2)\n\n\nClassification tree:\nrpart(formula = High ~ ., data = data1, parms = list(loss = matrix(c(0, \n    5, 1, 0), ncol = 2)), cp = 0.01, minsplit = 2)\n\nVariables actually used in tree construction:\n[1] Advertising Age         CompPrice   Education   Income      Population \n[7] Price       ShelveLoc  \n\nRoot node error: 236/400 = 0.59\n\nn= 400 \n\n         CP nsplit rel error xerror    xstd\n1  0.101695      0   1.00000 5.0000 0.20840\n2  0.050847      2   0.79661 3.8136 0.20909\n3  0.036017      3   0.74576 3.2034 0.20176\n4  0.035311      5   0.67373 3.1271 0.20038\n5  0.025424      9   0.50847 2.6144 0.19069\n6  0.016949     11   0.45763 2.3475 0.18307\n7  0.015537     16   0.37288 2.1992 0.17905\n8  0.014831     21   0.28814 2.1992 0.17905\n9  0.010593     23   0.25847 2.0466 0.17367\n10 0.010000     25   0.23729 2.0297 0.17292\n\n\n\nLe critère est ici modifié, on utilise une erreur de classification pondérée pour choisir l’arbre. On rappelle que l’erreur de classification est définie par \\[L(g)=P(g(X)\\neq Y)=E[\\alpha_11_{g(X)=0,Y=1}+\\alpha_21_{g(X)=1,Y=0}]\\] avec \\(\\alpha_1=\\alpha_2=1\\). Cette erreur donne donc le même poids aux deux erreurs possible (prédire 1 à tort ou prédire 0 à tort). Utiliser cette erreur revient donc à supposer qu’elles ont la même importance pour le problème considéré. Ce n’est bien entendu pas toujours le cas en pratique. La matrice loss contient les valeurs de \\(\\alpha_1\\) et \\(\\alpha_2\\) et modifier ces valeurs permettra de donner des poids différents à ces deux erreurs.\nAvec cette nouvelle commande, on donne un poids de 5 pour une erreur et de 1 pour l’autre. On obtient le terme demandé avec\n\nComparer les valeurs ajustées par les deux arbres considérés.\n\nIl y a plus de Yes prédits dans le second arbre. Cela vient des changements dans la matrice loss : la perte pour prédire No au lieu de Yes est de 5 pour le second arbre. Cela signifie bien détecter les Yes est plus important pour cet arbre, c’est donc tout à fait normal qu’il prédise plus souvent Yes que le premier.\nCette stratégie de changer la matrice de coût peut se révéler intéressante dans le cas de données déséquilibrées : une modalité de la cible sous-représentée par rapport à l’autre. En effet, pour de tels problèmes il est souvent très important de bien détecter la modalité sous-représentée. On pourra donc donner un poids plus fort lorsqu’on détecte mal cette modalité.\n\n\n\n\n1.2.3 Calcul de la sous-suite d’arbres optimaux\n\nExercice 1.1 (Minimisation du critère coût/complexité) On considère l’algorithme qui permet de calculer les suites \\((\\alpha_m)_m\\) et \\((T_{\\alpha_m})_m\\) du théorème présenté en cours. Pour simplifier on se place en classification binaire et on considère les notations suivantes (en plus de celles présentées dans le chapitre) :\n\n\\(R(t)\\) : erreur de classification dans le nœud \\(t\\) pondérée par la proportion d’individus dans le nœud (nombre d’individus dans \\(t\\) sue le nombre total d’individus).\n\\(T^t\\) : la branche de l’arbre \\(T\\) issue du nœud interne \\(t\\).\n\\(R(T^t)\\) : l’erreur de la branche \\(T^t\\) pondérée par la proportion d’individus dans le nœud.\n\nL’algorithme suivant présente le calcul explicite des suites \\((\\alpha_m)_m\\) et \\((T_{\\alpha_m})_m\\).\n\nInitialisation : on pose \\(\\alpha_0=0\\) et on calcule l’arbre maximale \\(T_0\\) qui minimise \\(C_0(T)\\). On fixe \\(m=0\\). Répéter jusqu’à obtenir l’arbre racine\n\nCalculer pour tous les nœuds \\(t\\) internes de \\(T_{\\alpha_m}\\) \\[g(t)=\\frac{R(t)-R(T_{\\alpha_m}^t)}{|{T_{\\alpha_m}^t}|-1}\\]\nChoisir le nœud interne \\(t_m\\) qui minimise \\(g(t)\\).\nOn pose \\[\\alpha_{m+1}=g(t_m)\\quad\\text{et}\\quad T_{\\alpha_{m+1}}=T_{\\alpha_m}-T_{\\alpha_m}^{t_m}.\\]\nMise à jour : \\(m:=m+1\\).\n\nRetourner : les suites finies \\((\\alpha_m)_m\\) et \\((T_{\\alpha_m})_m\\).\n\nOn propose d’utiliser cet algorithme sur l’arbre construit suivant\n\ngen_class_bin2D &lt;- function(n=100,graine=1234,bayes=0.1){\n  set.seed(graine)\n  grille &lt;- 0.1\n  X1 &lt;- runif(n)\n  X2 &lt;- runif(n)\n  Y &lt;- rep(0,n)\n  cond0 &lt;- (X1&gt;0.2 & X2&gt;=0.8) | (X1&gt;0.6 & X2&lt;0.4) | (X1&lt;0.25 & X2&lt;0.5)\n  cond1 &lt;- !cond0\n  Y[cond0] &lt;- rbinom(sum(cond0),1,bayes)\n  Y[cond1] &lt;- rbinom(sum(cond1),1,1-bayes)\n  donnees &lt;- tibble(X1,X2,Y=as.factor(Y))\n  px1 &lt;- seq(0,1,by=grille)\n  px2 &lt;- seq(0,1,by=grille)\n  px &lt;- expand.grid(X1=px1,X2=px2)\n  py &lt;- rep(0,nrow(px))\n  cond0 &lt;- (px[,1]&gt;0.2 & px[,2]&gt;=0.8) | \n    (px[,1]&gt;0.6 & px[,2]&lt;0.4) | \n    (px[,1]&lt;0.25 & px[,2]&lt;0.5)\n  cond1 &lt;- !cond0\n  py[cond0] &lt;- 0\n  py[cond1] &lt;- 1\n  df &lt;- px |&gt; as_tibble() |&gt; mutate(Y=as.factor(py))\n  p &lt;- ggplot(df)+aes(x=X1,y=X2,fill=Y)+\n    geom_raster(hjust=1,vjust=1)\n  return(list(donnees=donnees,graphe=p))\n}\ndon.2D.arbre &lt;- gen_class_bin2D(n=150,graine=3210,bayes=0.05)$donnees\nset.seed(123)\nT0 &lt;- rpart(Y~.,data=don.2D.arbre)\nrpart.plot(T0,extra = 1)\n\n\n\n\nL’arbre \\(T_0\\).\n\n\n\n\nCet arbre n’est pas l’arbre maximal mais la manière d’élaguer est identique.\n\nCalculer pour les 5 nœuds internes de \\(T_0\\) la fonction \\(g(t)\\).\n\nOn numérote les nœuds internes de haut en bas et de gauche à droite. On commence par le nœud \\(t_5\\), celui qui correspond à la coupure X1&gt;=0.31. On a \\[R(t_5)=\\frac{4}{23}\\ \\frac{23}{150}=\\frac{4}{150}\\quad\\text{et}\\quad R(T_0^{t_5})=\\frac{3}{23}\\ \\frac{23}{150}=\\frac{3}{150}.\\] On déduit \\[g(t_5)=\\frac{4/150-3/150}{2-1}=\\frac{1}{150}.\\] On fait de même pour les 4 autres nœuds internes et on obtient les résultats suivants :\n\n\n\nt\n\\(t_1\\)\n\\(t_2\\)\n\\(t_3\\)\n\\(t_4\\)\n\\(t_5\\)\n\n\n\n\n\\(R(t)\\)\n65/150\n20/150\n22/150\n12/150\n4/150\n\n\n\\(R(T_0^{t})\\)\n8/150\n2/150\n6/150\n1/150\n3/150\n\n\n\\(|{T_0^t}|\\)\n6\n3\n3\n2\n2\n\n\n\\(g(t)\\)\n11.4/150\n9/150\n8/150\n11/150\n1/150\n\n\n\n\nEn déduire la valeur de \\(\\alpha_1\\) ainsi que l’arbre \\(T_{\\alpha_1}\\).\n\n\\(g(t)\\) est minimum en \\(t_5\\) On a donc \\(\\alpha_1=g(t_5)\\) et \\(T_{\\alpha_1}=T_0-T_0^{t_5}\\), c’est-à-dire \\(T_0\\) auquel on enlève la coupure X1&gt;=0.31.\n\nRetrouver cette valeur en utilisant la fonction printcp et représenter l’arbre \\(T_1\\) en utilisant prune.\n\nOn se rappelle que printcp normalise toutes les erreurs par rapport à celle de l’arbre racine, par conséquent la valeur de \\(\\alpha_1\\) affichée par printcp sera \\[\\frac{1}{150} \\, \\frac{150}{65}=\\frac{1}{65}\\approx 0.01538.\\] En effet :\n\n\nEt on peut visualiser l’arbre avec\n\nFaire le même travail pour calculer \\(\\alpha_2\\) et \\(T_{\\alpha_2}\\).\n\nOn se place maintenant dans \\(T_{\\alpha_1}\\) qui contient 4 nœuds internes et on calcule \\(g(t)\\) pour ces 4 nœuds :\n\n\n\nt\n\\(t_1\\)\n\\(t_2\\)\n\\(t_3\\)\n\\(t_4\\)\n\n\n\n\n\\(R(t)\\)\n65/150\n20/150\n22/150\n12/150\n\n\n\\(R(T_{\\alpha_1}^t)\\)\n9/150\n18/150\n7/150\n1/150\n\n\n\\(|{T_{\\alpha_1}^t}|\\)\n5\n3\n2\n2\n\n\n\\(g(t)\\)\n14/150\n9/150\n15/150\n11/150\n\n\n\nOn supprimera ici \\(t_2\\) avec on posera \\(\\alpha_2=9/65\\approx 0.13846\\) (on normalise). On peut tracer \\(T_{\\alpha_2}\\) :\n\n\n\n\n\n\n\nBreiman, L., J. Friedman, R. Olshen, et C. Stone. 1984. Classification and regression trees. Wadsworth & Brooks."
  },
  {
    "objectID": "02-forets.html",
    "href": "02-forets.html",
    "title": "2  Forêts aléatoires",
    "section": "",
    "text": "Les méthodes par arbres présentées précédemment sont des algorithmes qui possèdent tout un tas de qualités (facile à mettre en œuvre, interprétable…). Ce sont néanmoins rarement les algorithmes qui se révèlent les plus performants. Les méthodes d’agrégation d’arbres présentées dans cette partie sont souvent beaucoup plus pertinentes, notamment en terme de qualité de prédiction. Elles consistent à construire un très grand nombre d’arbres “simples” : \\(g_1,\\dots,g_B\\) et à les agréger en faisant la moyenne : \\[\\frac{1}{B}\\sum_{k=1}^Bg_k(x).\\] Les forêts aléatoires (Breiman 2001) et le gradient boosting (Friedman 2001) utilisent ce procédé d’agrégation. Dans ce chapitre on étudiera ces algorithmes sur le je de données spam :\n\nlibrary(kernlab)\ndata(spam)\nset.seed(1234)\nspam &lt;- spam[sample(nrow(spam)),]\n\nLe problème est d’expliquer la variable binaire type par les autres.\nL’algorithme des forêts aléatoires consiste à construire des arbres sur des échantillons bootstrap et à les agréger. Il peut s’écrire de la façon suivante :\n\nEntrées :\n\n\\(x\\in\\mathbb R^d\\) l’observation à prévoir, \\(\\mathcal D_n\\) l’échantillon ;\n\\(B\\) nombre d’arbres ; \\(n_{max}\\) nombre max d’observations par nœud\n\\(m\\in\\{1,\\dots,d\\}\\) le nombre de variables candidates pour découper un nœud.\n\nAlgorithme : pour \\(k=1,\\dots,B\\) :\n\nTirer un échantillon bootstrap dans \\(\\mathcal D_n\\)\nConstruire un arbre CART sur cet échantillon bootstrap, chaque coupure est sélectionnée en minimisant la fonction de coût de CART sur un ensemble de \\(m\\) variables choisies au hasard parmi les \\(d\\). On note \\(T(.,\\theta_k,\\mathcal D_n)\\) l’arbre construit.\n\nSortie : l’estimateur \\(T_B(x)=\\frac{1}{B}\\sum_{k=1}^BT(x,\\theta_k,\\mathcal D_n)\\).\n\nCet algorithme peut être utilisé sur R avec la fonction randomForest du package randomForest ou la fonction ranger du package ranger.\n\nExercice 2.1 (Biais et variance des algorithmes bagging) Comparer le biais et la variance de la forêt \\(T_B(x)\\) au biais et à la variance d’un arbre de la forêt \\(T(x,\\theta_k,\\mathcal D_n)\\). On pourra utiliser \\(\\rho(x)=\\text{corr}(T(x,\\theta_1,\\mathcal D_n),T(x,\\theta_2,\\mathcal D_n))\\) pour comparer les variances.\n\nPour simplifier les notations on considère \\(T_1,\\dots,T_B\\) \\(B\\) variables aléatoires de même loi et de variance \\(\\sigma^2\\). Il est facile de voir que \\(\\mathbf E[\\bar T]=\\mathbf E[T_1]\\). Pour la variance on a\n\\[\n\\begin{aligned}\n\\mathbf V[\\bar T]= & \\frac{1}{B^2}\\mathbf V\\left[\\sum_{i=1}^BT_i\\right]= \\frac{1}{B^2}\\left[\\sum_{i=1}^V\\mathbf V[T_i]+\\sum_{i\\neq j}\\mathbf{cov}(T_i,T_j)\\right] \\\\\n= & \\frac{1}{B^2}\\left[B\\sigma^2+B(B-1)\\rho\\sigma^2\\right]=\\rho\\sigma^2+\\frac{1-\\rho}{B}\\sigma^2.\n\\end{aligned}\n\\]\nConsidérons \\(\\rho\\leq 0\\). On déduit de l’équation précédente que \\(B\\leq 1-1/\\rho\\). Par exemple si \\(\\rho=-1\\), \\(B\\) doit être inférieur ou égal à 2. Il n’est en effet pas possible de considérer 3 variables aléatoires de même loi dont les corrélations 2 à 2 sont égales à -1. De même si \\(\\rho=-1/2\\), \\(B\\leq 3\\)…\n\n\n\nExercice 2.2 (RandomForest versus ranger) On sépare le jeu de données spam en un échantillon d’apprentissage et un échantillon test :\n\nset.seed(1234)\nlibrary(tidymodels)\ndata_split &lt;- initial_split(spam, prop = 3/4)\nspam.app &lt;- training(data_split)\nspam.test  &lt;- testing(data_split)\n\n\nEntraîner une forêt aléatoire sur les données d’apprentissage uniquement en utilisant les paramètres par défaut de la fonction randomForest. Commenter.\n\nIl s’agit d’une forêt de classification avec 500 arbres. Le paramètre mtry vaut 7 et l’erreur OOB est de 4.72%.\n\nCalculer les groupes prédits pour les individus de l’échantillon test et en déduire une estimation de l’erreur de classification.\nCalculer les estimations de la probabilité de spam pour les individus de l’échantillon test.\nRefaire les questions précédentes avec la fonction ranger du package ranger (voir https://arxiv.org/pdf/1508.04409.pdf).\n\nlibrary(ranger)\n\n\nSi on souhaite estimer les probabilités d’être (ou pas) spam, il faut le spécifier dans la construction de la forêt :\n\nComparer les temps de calcul de randomForest et ranger.\n\nranger est beaucoup plus rapide.\n\n\n\n\nExercice 2.3 (Sélection des paramètres) Nous nous intéressons ici au choix des paramètres de la forêt aléatoire.\n\nExpliquer la sortie suivante.\n\nset.seed(12345)\nlibrary(OOBCurve)\nforet1 &lt;- ranger(type~.,data=spam,keep.inbag=TRUE)\nspam.task &lt;- mlr::makeClassifTask(data=spam,target=\"type\")\nerreurs &lt;- OOBCurve(foret1,measures = list(mmce, auc),\n                task=spam.task,data=spam)\nerreurs1 &lt;- erreurs |&gt; as_tibble() |&gt; mutate(ntrees=1:500) |&gt; \n  filter(ntrees&gt;=5) |&gt;\n  pivot_longer(-ntrees,names_to=\"Erreur\",values_to=\"valeur\")  \nggplot(erreurs1)+aes(x=ntrees,y=valeur)+geom_line()+\n  facet_wrap(~Erreur,scales=\"free\")\n\n\n\n\n\nCe graphe permet de visualiser l’évolution des erreurs OOB (AUC et erreur de classification) en fonction du nombre d’arbres. Il peut être utilisé pour voir si l’algorithme a bien “convergé”. Si ce n’est pas le cas, il faut construire une forêt avec plus d’arbres.\n\nConstruire la forêt avec mtry=1 et comparer ses performances avec celle construite précédemment.\n\nLa forêt foret1 est plus performante en terme d’erreur de classification OOB.\n\nA l’aide des outils tidymodels sélectionner les paramètres mtry et min_n dans les grilles c(1,6,seq(10,50,by=10),57) et c(1,5,100,500). On pourra notamment visualiser les critères en fonction des valeurs de paramètres.\n\nOn commence par construire la grille :\n\n\nOn définit le workflow\n\n\nOn effectue la validation croisée en parallélisant :\n\n\nOn étudie les meilleures valeurs de paramètres pour les deux critères considérés :\n\n\nOn visualise les erreurs de classification\n\n\net les AUC\n\n\nOn retrouve bien des petites valeurs pour min_n : il faut des arbres profonds pour que la forêt soit performante. Les valeurs optimales de mtry se situent autours de la valeur par défaut (7 ici). On peut donc conserver cette valeur pour ré-ajuster la forêt sur toutes les données :\n\nVisualiser l’importance des variables pour les scores d’impureté et de permutations.\n\n\n\nExercice 2.4 (Arbre vs forêt aléatoire) Proposer et mettre en œuvre une procédure permettant de comparer les performances (courbes ROC, AUC et accuracy) d’un arbre CART utilisant la procédure d’élagage proposée dans la Section 1.2 avec une forêt aléatoire.\n\nOn peut envisager différentes stratégies pour répondre à cette question. Il convient de bien préciser ce que l’on souhaite faire. Il ne s’agit pas de sélectionner les paramètres d’un algorithme. On souhaite comparer deux algorithmes de prévision :\n\nun arbre CART qui utilise la procédure d’élagage CART : création de la suite optimale de sous arbre puis sélection d’un arbre dans cette suite en estimant l’erreur de classification par validation croisée ;\nune forêt aléatoire qui prend les valeurs par défaut pour nodesize et qui sélection mtry en minimisant l’erreur OOB (c’est un choix).\n\nIl faut estimer les risques demandés en se donnant une stratégie de ré-échantillonnage. On choisit une validation croisée 10 blocs :\n\n\nOn crée une fonction spécifique à chaque algorithme qui calculera les prévisions de nouveaux individus :\n\n\nOn effectue la validation croisée :\n\n\nOn déduit la courbe ROC, l’AUC\n\n\net l’accuracy\n\n\n\n\n\n\nBreiman, L. 2001. « Random forests ». Machine learning 45: 5‑32.\n\n\nFriedman, J. H. 2001. « Greedy Function Approximation: A Gradient Boosting Machine ». Annals of Statistics 29: 1189‑1232."
  },
  {
    "objectID": "03-boosting.html#sec-sinusgbm",
    "href": "03-boosting.html#sec-sinusgbm",
    "title": "3  Gradient boosting",
    "section": "3.1 Un exemple simple en régression",
    "text": "3.1 Un exemple simple en régression\nOn considère un jeu de données \\((x_i,y_i),i=1,\\dots,200\\) issu d’un modèle de régression \\[y_i=m(x_i)+\\varepsilon_i\\] où la vraie fonction de régression est la fonction sinus (mais on va faire comme si on ne le savait pas).\n\nx &lt;- seq(-2*pi,2*pi,by=0.01)\ny &lt;- sin(x)\nset.seed(1234)\nX &lt;- runif(200,-2*pi,2*pi)\nY &lt;- sin(X)+rnorm(200,sd=0.2)\ndf1 &lt;- data.frame(X,Y)\ndf2 &lt;- data.frame(X=x,Y=y)\np1 &lt;- ggplot(df1)+aes(x=X,y=Y)+geom_point()+\n  geom_line(data=df2,linewidth=1)+xlab(\"\")+ylab(\"\")\np1\n\n\n\n\n\nRappeler ce que siginifie le \\(L_2\\)-boosting.\n\nIl s’agit de l’algorithme de gradient boosting présenté ci-dessus appliqué à la fonction de perte \\[\\ell(y,f(x))=\\frac{1}{2}(y-f(x))^2.\\]\n\nA l’aide de la fonction gbm du package gbm construire un algorithme de \\(L_2\\)-boosting. On utilisera 500000 itérations et gardera les autres valeurs par défaut de paramètres, à l’exception de bag.fraction qu’on prendra égal à 1.\n\nlibrary(gbm)\nL2boost &lt;- gbm(Y~.,data=df1,...)\n\nVisualiser l’estimateur à la première itération. On pourra faire un predict avec l’option n.trees ou utiliser directement la fonction plot.gbm avec l’option n.trees.\n\nhelp(predict.gbm)\nprev1 &lt;- predict(L2boost,...)\n#ou\nhelp(plot.gbm)\nplot(L2boost,...)\n\n\nOn remarque que l’estimateur est un arbre avec une seule coupure. On aurait aussi pu utiliser :\n\nFaire de même pour les itérations 1000 et 500000.\n\nprev1000 &lt;- predict(L2boost,newdata=df2,...)\nprev500000 &lt;- predict(L2boost,newdata=df2,...)\n...\n\n\nOn sur-ajuste lorsque le nombre d’itérations est trop important.\n\nSélectionner le nombre d’itérations par la procédure de votre choix.\n\nOn propose de faire une validation hold out. C’est assez facile avec gbm il suffit de renseigner l’option train.fraction de gbm.\n\n\nL2boost.sel &lt;- gbm(Y~.,data=df1,...)\ngbm.perf(...)\n\nReprésenter l’estimateur sélectionné.\n\nprev_opt &lt;- predict(L2boost.sel,newdata=df2,...)\n..."
  },
  {
    "objectID": "03-boosting.html#sec-gbmspam",
    "href": "03-boosting.html#sec-gbmspam",
    "title": "3  Gradient boosting",
    "section": "3.2 Adaboost et logitboost pour la classification binaire.",
    "text": "3.2 Adaboost et logitboost pour la classification binaire.\nOn considère le jeu de données spam du package kernlab.\n\nlibrary(kernlab)\ndata(spam)\nset.seed(1234)\nspam &lt;- spam[sample(nrow(spam)),]\n\n\nExécuter la commande et commenter la sortie.\n\nmodel_ada1 &lt;- gbm(type~.,data=spam,distribution=\"adaboost\",\n                  interaction.depth=2,\n                  shrinkage=0.05,n.trees=500)\n\n\nOn obtient le message d’erreur suivant :\n\nProposer une correction permettant de faire fonctionner l’algorithme.\n\nIl est nécessaire que la variable qualitative à expliquer soit codée 0-1 pour adaboost.\n\n\nspam1 &lt;- spam\nspam1$type &lt;- ...\n...    \n\nExpliciter le modèle ajusté par la commande précédente.\n\nL’algorithme gbm est une descente de gradient qui minimise la fonction de perte \\[\\frac{1}{n}\\sum_{i=1}^n \\ell(y_i,g(x_i)).\\] Dans le cas de adaboost on utilise la perte exponentielle : \\(\\ell(y,g(x))=\\exp(-yg(x))\\).\n\nEffectuer un summary du modèle ajusté. Expliquer la sortie.\n\nOn obtient un indicateur qui permet de mesurer l’importance des variable dans la construction de la méthode.\n\nUtiliser la fonction vip du package vip pour retrouver ce sorties.\nSélectionner le nombre d’itérations pour l’algorithme adaboost en faisant une validation croisée 5 blocs.\nPour l’estimateur sélectionné, calculer la prévision (label et probabilité d’être un spam) de l’individu suivant :\n\nxnew &lt;- spam[1000,-58]\n\n\nOn obtient la probabilité d’être spam avec\n\n\nOn prédira donc nonspam.\n\nFaire la même procédure en changeant la valeur du paramètre shrinkage (par exemple 0.05 et 0.5). Interpréter.\n\nLe nombre d’itérations optimal augmente lorsque shrinkage diminue. C’est logique car ce dernier paramètre contrôle la vitesse de descente de gradient : plus il est grand, plus on minimise vite et moins on itère. Il faut néanmoins veiller à ne pas le prendre trop petit pour avoir un estimateur stable. Ici, 0.05 semble être une bonne valeur.\n\nExpliquer la différence entre adaboost et logitboost et précisez comment on peut mettre en œuvre ce dernier algorithme.\n\nLa seule différence se situe au niveau de la fonction de perte, adaboost utilise \\[\\exp(-yg(x))\\] tandis que logitboost utilise \\[\\log(1+\\exp(-2yg(x)))\\] Avec gbm il faudra utiliser l’option distribution=\"bernoulli\" pour faire du logitboost, par exemple :"
  },
  {
    "objectID": "03-boosting.html#comparaison-de-méthodes",
    "href": "03-boosting.html#comparaison-de-méthodes",
    "title": "3  Gradient boosting",
    "section": "3.3 Comparaison de méthodes",
    "text": "3.3 Comparaison de méthodes\nOn reprend les données spam de l’exercice précédent et on les coupe en un échantillon d’apprentissage pour entraîner les algorithmes et un échantillon test pour les comparer :\n\nset.seed(1234)\nperm &lt;- sample(1:nrow(spam))\napp &lt;- spam[perm[1:3000],]\ntest &lt;- spam[-perm[1:3000],]\n\n\nSur les données d’apprentissage uniquement, entraîner\n\nl’algorithme adaboost en sélectionnant le nombre d’itérations par validation croisée\nl’algorithme logitboost en sélectionnant le nombre d’itérations par validation croisée\nune forêt aléatoire avec les paramètres par défaut\nun arbre CART\n\n\napp1 &lt;- app |&gt; mutate(type=as.numeric(type)-1)\nmodel_ada &lt;- gbm(type~.,data=app1,...)\nnb_ada &lt;- gbm.perf(...)\n\n\nmodel_logit &lt;- gbm(type~.,data=app1,...)\nnb_logit &lt;- gbm.perf(...)\n\n\nlibrary(ranger)\nforet &lt;- ranger(...,probability=TRUE)\nlibrary(rpart)\narbre &lt;- rpart(...)\ncp_opt &lt;- ...\narbre.opt &lt;- prune(...)\n\nPour les 4 algorithmes, calculer, pour tous les individus de l’échantillon test, la probabilité que ce soit un spam. On pourra stocker toutes ces probabilités dans un même tibble.\n\nprob &lt;- tibble(ada=predict(...,type=\"response\"),\n               logit=predict(...),\n               foret=predict(...)$prediction[,2],\n               arbre=predict(...)[,2],\n               obs=test$type)\n\nComparer les 3 algorithmes avec la courbe ROC, l’AUC et l’erreur de classification.\nComment aurait-on pu faire pour obtenir des résultats plus précis ?\n\nAvec une validation croisée plutôt qu’un simple découpage apprentissage/test."
  },
  {
    "objectID": "03-boosting.html#xgboost",
    "href": "03-boosting.html#xgboost",
    "title": "3  Gradient boosting",
    "section": "3.4 Xgboost",
    "text": "3.4 Xgboost\nL’algorithme xgboost Chen et Guestrin (2016) va plusloin que le gradient boosting en minimisant une approximation à l’odre 2 de la fonction de perte et en ajoutant un terme de régularisation dans la fonction objectif. On cherche toujours des combinaisons d’arbres\n\\[f_b(x)=f_{b-1}(x)+h_b(x)\\quad\\text{où}\\quad h_b(x)=w_{q(x)}\\]\nest un arbre à \\(T\\) feuilles : \\(w\\in\\mathbb R^T\\) et \\(q:\\mathbb R^d\\to\\{1,2,\\dots,T\\}\\). À l’étape \\(b\\), on cherche l’arbre qui minimise la fonction objectif de la forme\n\\[\n    \\begin{aligned}\n      \\text{obj}^{(b)}= & \\sum_{i=1}^n\\ell(y_i,f_b(x_i))+\\sum_{j=1}^b\\Omega(h_j) \\\\\n      = & \\sum_{i=1}^n\\ell(y_i,f_{b-1}(x_i)+h_b(x_i))+\\sum_{j=1}^b\\Omega(h_j)\n    \\end{aligned}\n\\]\noù \\(\\Omega(h_j)\\) est un terme de régularisation qui va pénaliser \\(h_j\\) en fonction de son nombre de feuilles \\(T\\) et des valeurs prédites \\(w\\). Un développement limité à l’ordre 2 permet d’approcher cette fonction par\n\\[\\text{obj}^{(b)}=\\sum_{i=1}^n[\\ell_i^{(1)}h_b(x_i)+\\frac{1}{2}\\ell_i^{(2)}h_b^2(x_i)]+\\Omega(h_b)+\\text{constantes},\\] avec\n\\[\\ell_i^{(1)}=\\frac{\\partial \\ell(y_i,f(x))}{\\partial f(x)}(f_{b-1}(x_i))\\quad\\text{et}\\quad \\ell_i^{(2)}=\\frac{\\partial^2 \\ell(y_i,f(x))}{\\partial f(x)^2}(f_{b-1}(x_i)).\\] Pour les arbres, la fonction de régularisation a la forme suivante :\n\\[\\Omega(h)=\\Omega(T,w)=\\gamma T+\\frac{1}{2}\\lambda\\sum_{j=1}^Tw_j^2,\\] où \\(\\gamma\\) et \\(\\lambda\\) contrôlent le poids que l’on donne aux paramètres de l’arbre. On obtient au final l’algorithme suivant\n\n\nInitialisation \\(f_0=h_0\\).\nPour \\(b=1,\\dots,B\\)\n\nAjuster un arbre \\(h_b\\) à \\(T\\) feuilles qui minimise \\[\\sum_{i=1}^n[\\ell_i^{(1)}h_b(x_i)+\\frac{1}{2}\\ell_i^{(2)}h_b^2(x_i)]+\\gamma T+\\frac{1}{2}\\lambda\\sum_{j=1}^Tw_j.\\]\nMettre à jour \\[f_b(x)=f_{b-1}(x)+h_b(x).\\]\n\nSortie : la suite d’algorithmes \\((f_b)_b\\).\n\n\nOn pourra trouver plus de précisions ici : https://xgboost.readthedocs.io/en/stable/tutorials/index.html\n\nExercice 3.1 (Prise en main des principales fonction de xgboost) On commence par charger le package\n\nlibrary(xgboost)\n\net on reprend les données sur le sinus de la Section 3.1 :\n\nx &lt;- seq(-2*pi,2*pi,by=0.01)\ny &lt;- sin(x)\nset.seed(1234)\nX &lt;- runif(200,-2*pi,2*pi)\nY &lt;- sin(X)+rnorm(200,sd=0.2)\ndf1 &lt;- data.frame(X,Y)\ndf2 &lt;- data.frame(X=x,Y=y)\n\nLa fonction xgboost requiert que les données possèdent la classe xgb.DMatrix, on peut l’obtenir avec\n\nX_mat &lt;- xgb.DMatrix(as.matrix(df1[,1]),label=df1$Y)\n\n\nExpliquer la sortie\n\nboost1 &lt;- xgboost(data=X_mat,nrounds = 5,\n                  params=list(objective=\"reg:squarederror\"))\n\n[1] train-rmse:0.633250 \n[2] train-rmse:0.468674 \n[3] train-rmse:0.354599 \n[4] train-rmse:0.275842 \n[5] train-rmse:0.224803 \n\nboost1\n\n##### xgb.Booster\nraw: 16.5 Kb \ncall:\n  xgb.train(params = params, data = dtrain, nrounds = nrounds, \n    watchlist = watchlist, verbose = verbose, print_every_n = print_every_n, \n    early_stopping_rounds = early_stopping_rounds, maximize = maximize, \n    save_period = save_period, save_name = save_name, xgb_model = xgb_model, \n    callbacks = callbacks)\nparams (as set within xgb.train):\n  objective = \"reg:squarederror\", validate_parameters = \"TRUE\"\nxgb.attributes:\n  niter\ncallbacks:\n  cb.print.evaluation(period = print_every_n)\n  cb.evaluation.log()\nniter: 5\nnfeatures : 1 \nevaluation_log:\n iter train_rmse\n    1  0.6332497\n    2  0.4686737\n    3  0.3545990\n    4  0.2758424\n    5  0.2248031\n\n\n\nOn a ici entraîné xgboost avec la fonction de perte quadratique et 5 itérations.\n\nFaire la même chose avec 100 itération et un learning rate de 0.1.\nOn peut obtenir les valeurs prédites entre \\(-2\\pi\\) et \\(2\\pi\\) pour 100 itérations avec\n\nXtest &lt;- as.matrix(df2$X)\nprev100 &lt;- predict(boost2,newdata=Xtest,iterationrange = c(1,101))\n\nTracer les estimateurs xgboost pour 1 itération, 20 itérations et 100 itérations. Commenter.\n\nComme pour le gradient boosting, l’algorithme sous-apprend si le nombre d’arbres est trop petit et sur-apprend lorsqu’il est trop grand.\n\nCommenter la sortie\n\nset.seed(123)\nsel.xgb &lt;- xgb.cv(data = X_mat,\n                  nrounds = 100, objective = \"reg:squarederror\", \n                  eval_metric = \"rmse\",\n                  nfold=5,eta=0.1,\n                  early_stopping_rounds=10,\n                  verbose=FALSE)\nsel.xgb$evaluation_log |&gt; head()\n\n   iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std\n1:    1       0.7894653    0.012973111      0.7906004    0.05513590\n2:    2       0.7189852    0.011889426      0.7228767    0.05364182\n3:    3       0.6556685    0.010877136      0.6607388    0.05284791\n4:    4       0.5985454    0.010104324      0.6055111    0.05240736\n5:    5       0.5471973    0.009379501      0.5567722    0.05117564\n6:    6       0.5007942    0.008971210      0.5129458    0.04971156\n\n(ite.opt.xgb &lt;- sel.xgb$best_iteration)\n\n[1] 27\n\nsel.xgb$niter\n\n[1] 37\n\n\n\nOn effectue une validation croisée 5 blocs pour choisir le nombre d’itérations. L’argument early_stopping_rounds=10 permet de stopper l’algorithme lorsque l’erreur de prévision commence à trop remonter.\n\nTracer les prévisions pour l’algorithme sélectionné.\n\n\n\nExercice 3.2 (Xgboost sur les données spam) On reprend les données spam de la Section 3.2. Entraîner un algorithme xgboost avec la fonction de perte binary:logistic et en sélectionnant la nombre d’itérations par validation croisée en optimisant l’AUC. Attention cette fonction de perte requiert que la variable à expliquer prenne pour valeurs 0 ou 1 en classe numeric.\n\nOn teste la fonction xgboost pour voir si les donées sont au bon format.\n\n\nOn peut maintenant faire la valisation croisée :\n\n\nOn récupère les prévisions issues de la validation croisée\n\n\npour tracer la courbe ROC et calculer l’AUC :\n\n\n\nExercice 3.3 (Sélection avec tidymodels) Refaire l’exercice précédent avec la syntaxe tidymodels. On choisira notamment :\n\nla profondeur des arbres dans le vecteur\n\nc(1,3,8,10)\n\nle nombre d’itérations entre 1 et 500 avec un early_stopping toujours égal à 10 et un learning rate à 0.05.\n\nOn pourra consulter la page https://www.tidymodels.org/find/parsnip/ pour trouver les noms de paramètre du worfklow et sur le tutoriel https://juliasilge.com/blog/shelter-animals/ pour la stratégie. Elle est ici de fixer le nombre d’itérations à 500 puisqu’on utilise le early stopping en séparant les données en 2. On initialisera donc le workflow avec\n\nlibrary(tidymodels)\ntune_spec &lt;- \n  boost_tree(tree_depth=tune(),trees=500,learn_rate=0.05,\n             stop_iter=10) |&gt; \n  set_mode(\"classification\") |&gt;\n  set_engine(\"xgboost\",validation=0.2) \n\nxgb_wf &lt;- workflow() |&gt;\n  add_model(tune_spec) |&gt;\n  add_formula(type ~ .)\n\n\nOn définit la grille de paramètres et le ré-échantillonnage :\n\n\nOn fait la validation croisée :\n\n\nOn visualise les résultats et on choisit la meilleure valeur :\n\n\nOn finit en entraînant l’algorithme sur toute les données pur la valeur choisie :\n\n\nOn peut retrouver le nombre d’itérations sélectionnés par early stopping avec\n\n\nOn visualise enfin l’importance des variables avec\n\n\n\n\n\n\nChen, T., et C. Guestrin. 2016. « XGBoost: A Scalable Tree Boosting System ». In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 785‑94. KDD ’16. New York, NY, USA: ACM. https://doi.org/10.1145/2939672.2939785.\n\n\nFriedman, J. H. 2001. « Greedy Function Approximation: A Gradient Boosting Machine ». Annals of Statistics 29: 1189‑1232.\n\n\nRidgeway, G. 2006. « Generalized boosted models: A guide to the gbm package »."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Références",
    "section": "",
    "text": "Breiman, L. 2001. “Random Forests.” Machine\nLearning 45: 5–32.\n\n\nBreiman, L., J. Friedman, R. Olshen, and C. Stone. 1984.\nClassification and Regression Trees. Wadsworth & Brooks.\n\n\nChen, T., and C. Guestrin. 2016. “XGBoost: A Scalable\nTree Boosting System.” In Proceedings of the 22nd ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining,\n785–94. KDD ’16. New York, NY, USA: ACM. https://doi.org/10.1145/2939672.2939785.\n\n\nFriedman, J. H. 2001. “Greedy Function Approximation: A Gradient\nBoosting Machine.” Annals of Statistics 29: 1189–1232.\n\n\nRidgeway, G. 2006. “Generalized Boosted Models: A Guide to the Gbm\nPackage.”"
  }
]